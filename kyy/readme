merge baseline

baseline: 0.93845  / 7055s
https://www.kaggle.com/chechir/bert-lstm-rank-blender

1. small bert model 120mil data : 0.93852 / 7110s
https://www.kaggle.com/kdafke123141/bert-lstm-rank-blender-small




3차 모임 base line

data augmentation sorce 
(https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038#latest-310689)
-too many requested




baseline + data augmentation ( 이전 대회 데이터 활용)

https://www.kaggle.com/kdafke123141/pretext-lstm-tuning-augmentation-except-for-ori/edit/run/15342750

