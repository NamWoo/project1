merge baseline


valinal

large
0.9285 (large maxlen220_epoch0)
0.9289 (large maxlen220_epoch1)

small 
0.9338 (samll maxlen300_epoch0)
0.9335 (samll maxlen300_epoch1)



lstm + bert : small_max_300_120mi
0_epoch : 0.93899 ( frist)
1_epoch : 0.93957 ( second)
https://www.kaggle.com/kdafke123141/bert-lstm-rank-blender-small


lstm + bert : small_max_300_120mi (weight: 0.66 + 0.33)
0.93975
https://www.kaggle.com/kdafke123141/bert-lstm-rank-blender-small-weight?scriptVersionId=15862427



jigsaw-starter-blend-base-max_300/epoch:2 : 0.93924


*lstm + bert : small_170mi_max_300_5e5
0_epoch : : 0.93913
1_epoch : ?





baseline: 0.93845  / 7055s
https://www.kaggle.com/chechir/bert-lstm-rank-blender

# ensemble
1. small bert model 120mil data : 0.93852 / 7110s
https://www.kaggle.com/kdafke123141/bert-lstm-rank-blender-small

2. large bert model 120mil data : out of memory?




# single model

1. bert small single model : 0.93373
(valia version : 0.9298784415544122) : howto improve?

(my own 120mil : 0.93413)

2. bert larger model : 0.92826
https://www.kaggle.com/kdafke123141/pytorch-bert-inference-large-model

3. bert larger model_epoch2 : 5 submition validation (after ...)



3차 모임 base line

data augmentation sorce 
(https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038#latest-310689)
-too many requested




baseline + data augmentation ( 이전 대회 데이터 활용)

https://www.kaggle.com/kdafke123141/pretext-lstm-tuning-augmentation-except-for-ori/edit/run/15342750

